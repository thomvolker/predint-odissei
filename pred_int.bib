
@article{neter_applied_1996,
	title = {Applied linear statistical models},
	url = {https://mubert.marshall.edu/bert/syllabi/310720160114404301635160.pdf},
	urldate = {2024-11-15},
	author = {Neter, John and Kutner, Michael H. and Nachtsheim, Christopher J. and Wasserman, William},
	year = {1996},
	note = {Publisher: Irwin Chicago},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/WR9LRJ5X/Neter et al. - 1996 - Applied linear statistical models.pdf:application/pdf},
}

@article{navarro_systematic_2023,
	title = {Systematic review identifies the design and methodological conduct of studies on machine learning-based prediction models},
	volume = {154},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435622003006},
	urldate = {2024-11-15},
	journal = {Journal of Clinical Epidemiology},
	author = {Navarro, Constanza L. Andaur and Damen, Johanna AA and van Smeden, Maarten and Takada, Toshihiko and Nijman, Steven WJ and Dhiman, Paula and Ma, Jie and Collins, Gary S. and Bajpai, Ram and Riley, Richard D.},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {8--22},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/5TIADE3C/S0895435622003006.html:text/html},
}

@article{little_regression_1992,
	title = {Regression with {Missing} \textit{{X}} 's: {A} {Review}},
	volume = {87},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Regression with {Missing} \textit{{X}} 's},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10476282},
	doi = {10.1080/01621459.1992.10476282},
	language = {en},
	number = {420},
	urldate = {2024-11-15},
	journal = {Journal of the American Statistical Association},
	author = {Little, Roderick J. A.},
	month = dec,
	year = {1992},
	pages = {1227--1237},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/AGM2G6GI/Little - 1992 - Regression with Missing X 's A Review.pdf:application/pdf},
}

@misc{amsterdam_causal_2024,
	title = {A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions},
	shorttitle = {A causal viewpoint on prediction model performance under changes in case-mix},
	url = {http://arxiv.org/abs/2409.01444},
	abstract = {Prediction models inform important clinical decisions, aiding in diagnosis, prognosis, and treatment planning. The predictive performance of these models is typically assessed through discrimination and calibration. However, changes in the distribution of the data impact model performance. In health-care, a typical change is a shift in case-mix: for example, for cardiovascular risk management, a general practitioner sees a different mix of patients than a specialist in a tertiary hospital. This work introduces a novel framework that differentiates the effects of case-mix shifts on discrimination and calibration based on the causal direction of the prediction task. When prediction is in the causal direction (often the case for prognosis predictions), calibration remains stable under case-mix shifts, while discrimination does not. Conversely, when predicting in the anti-causal direction (often with diagnosis predictions), discrimination remains stable, but calibration does not. A simulation study and empirical validation using cardiovascular disease prediction models demonstrate the implications of this framework. This framework provides critical insights for evaluating and deploying prediction models across different clinical settings, emphasizing the importance of understanding the causal structure of the prediction task.},
	urldate = {2024-11-16},
	publisher = {arXiv},
	author = {Amsterdam, Wouter A. C. van},
	month = sep,
	year = {2024},
	note = {arXiv:2409.01444},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology},
	file = {Preprint PDF:/Users/4243579/Zotero/storage/2II6F663/Amsterdam - 2024 - A causal viewpoint on prediction model performance.pdf:application/pdf;Snapshot:/Users/4243579/Zotero/storage/YLE9HDDF/2409.html:text/html},
}

@article{sisk_imputation_2023,
	title = {Imputation and missing indicators for handling missing data in the development and deployment of clinical prediction models: {A} simulation study},
	volume = {32},
	issn = {0962-2802, 1477-0334},
	shorttitle = {Imputation and missing indicators for handling missing data in the development and deployment of clinical prediction models},
	url = {https://journals.sagepub.com/doi/10.1177/09622802231165001},
	doi = {10.1177/09622802231165001},
	abstract = {Background: In clinical prediction modelling, missing data can occur at any stage of the model pipeline; development, validation or deployment. Multiple imputation is often recommended yet challenging to apply at deployment; for example, the outcome cannot be in the imputation model, as recommended under multiple imputation. Regression imputation uses a fitted model to impute the predicted value of missing predictors from observed data, and could offer a pragmatic alternative at deployment. Moreover, the use of missing indicators has been proposed to handle informative missingness, but it is currently unknown how well this method performs in the context of clinical prediction models. Methods: We simulated data under various missing data mechanisms to compare the predictive performance of clinical prediction models developed using both imputation methods. We consider deployment scenarios where missing data is permitted or prohibited, imputation models that use or omit the outcome, and clinical prediction models that include or omit missing indicators. We assume that the missingness mechanism remains constant across the model pipeline. We also apply the proposed strategies to critical care data. Results: With complete data available at deployment, our findings were in line with existing recommendations; that the outcome should be used to impute development data when using multiple imputation and omitted under regression imputation. When missingness is allowed at deployment, omitting the outcome from the imputation model at the development was preferred. Missing indicators improved model performance in many cases but can be harmful under outcome-dependent missingness. Conclusion: We provide evidence that commonly taught principles of handling missing data via multiple imputation may not apply to clinical prediction models, particularly when data can be missing at deployment. We observed comparable predictive performance under multiple imputation and regression imputation. The performance of the missing data handling method must be evaluated on a study-by-study basis, and the most appropriate strategy for handling missing data at development should consider whether missing data are allowed at deployment. Some guidance is provided.},
	language = {en},
	number = {8},
	urldate = {2024-11-16},
	journal = {Statistical Methods in Medical Research},
	author = {Sisk, Rose and Sperrin, Matthew and Peek, Niels and Van Smeden, Maarten and Martin, Glen Philip},
	month = aug,
	year = {2023},
	pages = {1461--1477},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/FQPU5LG2/Sisk et al. - 2023 - Imputation and missing indicators for handling mis.pdf:application/pdf},
}

@book{van_buuren_flexible_2018,
	title = {Flexible imputation of missing data},
	url = {https://books.google.com/books?hl=nl&lr=&id=lzb3DwAAQBAJ&oi=fnd&pg=PP1&dq=Flexible+Imputation+of+Missing+Data&ots=Vh2U_JhbX-&sig=Cv43tBLqwOf6_AMbIK-XHhSTV0k},
	urldate = {2024-11-16},
	publisher = {CRC press},
	author = {Van Buuren, Stef},
	year = {2018},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/M6GTP6FA/Van Buuren - 2018 - Flexible imputation of missing data.pdf:application/pdf},
}

@article{deforth_performance_2024,
	title = {The performance of prognostic models depended on the choice of missing value imputation algorithm: a simulation study},
	volume = {176},
	issn = {0895-4356},
	shorttitle = {The performance of prognostic models depended on the choice of missing value imputation algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435624002956},
	doi = {10.1016/j.jclinepi.2024.111539},
	abstract = {Objectives
The development of clinical prediction models is often impeded by the occurrence of missing values in the predictors. Various methods for imputing missing values before modeling have been proposed. Some of them are based on variants of multiple imputations by chained equations, while others are based on single imputation. These methods may include elements of flexible modeling or machine learning algorithms, and for some of them user-friendly software packages are available. The aim of this study was to investigate by simulation if some of these methods consistently outperform others in performance measures of clinical prediction models.
Study Design and Setting
We simulated development and validation cohorts by mimicking observed distributions of predictors and outcome variable of a real data set. In the development cohorts, missing predictor values were created in 36 scenarios defined by the missingness mechanism and proportion of noncomplete cases. We applied three imputation algorithms that were available in R software (R Foundation for Statistical Computing, Vienna, Austria): mice, aregImpute, and missForest. These algorithms differed in their use of linear or flexible models, or random forests, the way of sampling from the predictive posterior distribution, and the generation of a single or multiple imputed data set. For multiple imputation, we also investigated the impact of the number of imputations. Logistic regression models were fitted with the simulated development cohorts before (full data analysis) and after missing value generation (complete case analysis), and with the imputed data. Prognostic model performance was measured by the scaled Brier score, c-statistic, calibration intercept and slope, and by the mean absolute prediction error evaluated in validation cohorts without missing values. Performance of full data analysis was considered as ideal.
Results
None of the imputation methods achieved the model's predictive accuracy that would be obtained in case of no missingness. In general, complete case analysis yielded the worst performance, and deviation from ideal performance increased with increasing percentage of missingness and decreasing sample size. Across all scenarios and performance measures, aregImpute and mice, both with 100 imputations, resulted in highest predictive accuracy. Surprisingly, aregImpute outperformed full data analysis in achieving calibration slopes very close to one across all scenarios and outcome models. The increase of mice's performance with 100 compared to five imputations was only marginal. The differences between the imputation methods decreased with increasing sample sizes and decreasing proportion of noncomplete cases.
Conclusion
In our simulation study, model calibration was more affected by the choice of the imputation method than model discrimination. While differences in model performance after using imputation methods were generally small, multiple imputation methods as mice and aregImpute that can handle linear or nonlinear associations between predictors and outcome are an attractive and reliable choice in most situations.},
	urldate = {2024-11-22},
	journal = {Journal of Clinical Epidemiology},
	author = {Deforth, Manja and Heinze, Georg and Held, Ulrike},
	month = dec,
	year = {2024},
	keywords = {Prediction model, AregImpute, Complete case analysis, Mice, MissForest, Missing value imputation},
	pages = {111539},
	file = {ScienceDirect Snapshot:/Users/4243579/Zotero/storage/Y8S8PQ9Z/S0895435624002956.html:text/html},
}

@article{riley_minimum_2019,
	title = {Minimum sample size for developing a multivariable prediction model: {Part} {I} – {Continuous} outcomes},
	volume = {38},
	issn = {1097-0258},
	shorttitle = {Minimum sample size for developing a multivariable prediction model},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7993},
	doi = {10.1002/sim.7993},
	abstract = {In the medical literature, hundreds of prediction models are being developed to predict health outcomes in individuals. For continuous outcomes, typically a linear regression model is developed to predict an individual's outcome value conditional on values of multiple predictors (covariates). To improve model development and reduce the potential for overfitting, a suitable sample size is required in terms of the number of subjects (n) relative to the number of predictor parameters (p) for potential inclusion. We propose that the minimum value of n should meet the following four key criteria: (i) small optimism in predictor effect estimates as defined by a global shrinkage factor of ≥0.9; (ii) small absolute difference of ≤ 0.05 in the apparent and adjusted R2; (iii) precise estimation (a margin of error ≤ 10\% of the true value) of the model's residual standard deviation; and similarly, (iv) precise estimation of the mean predicted outcome value (model intercept). The criteria require prespecification of the user's chosen p and the model's anticipated R2 as informed by previous studies. The value of n that meets all four criteria provides the minimum sample size required for model development. In an applied example, a new model to predict lung function in African-American women using 25 predictor parameters requires at least 918 subjects to meet all criteria, corresponding to at least 36.7 subjects per predictor parameter. Even larger sample sizes may be needed to additionally ensure precise estimates of key predictor effects, especially when important categorical predictors have low prevalence in certain categories.},
	language = {en},
	number = {7},
	urldate = {2024-11-24},
	journal = {Statistics in Medicine},
	author = {Riley, Richard D. and Snell, Kym I.E. and Ensor, Joie and Burke, Danielle L. and Harrell Jr, Frank E. and Moons, Karel G.M. and Collins, Gary S.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7993},
	keywords = {continuous outcome, linear regression, minimum sample size, multivariable prediction model, R-squared},
	pages = {1262--1275},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/TB5LRG5S/Riley et al. - 2019 - Minimum sample size for developing a multivariable.pdf:application/pdf;Snapshot:/Users/4243579/Zotero/storage/3JGWIW7T/sim.html:text/html},
}

@article{zhang_random_2020,
	title = {Random {Forest} {Prediction} {Intervals}},
	volume = {74},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2019.1585288},
	doi = {10.1080/00031305.2019.1585288},
	abstract = {Random forests are among the most popular machine learning techniques for prediction problems. When using random forests to predict a quantitative response, an important but often overlooked challenge is the determination of prediction intervals that will contain an unobserved response value with a specified probability. We propose new random forest prediction intervals that are based on the empirical distribution of out-of-bag prediction errors. These intervals can be obtained as a by-product of a single random forest. Under regularity conditions, we prove that the proposed intervals have asymptotically correct coverage rates. Simulation studies and analysis of 60 real datasets are used to compare the finite-sample properties of the proposed intervals with quantile regression forests and recently proposed split conformal intervals. The results indicate that intervals constructed with our proposed method tend to be narrower than those of competing methods while still maintaining marginal coverage rates approximately equal to nominal levels.},
	number = {4},
	urldate = {2024-11-24},
	journal = {The American Statistician},
	author = {Zhang, Haozhe and Zimmerman, Joshua and Nettleton, Dan and Nordman, Daniel J.},
	month = oct,
	year = {2020},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/00031305.2019.1585288},
	pages = {392--406},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/Y8RJJ9PC/Zhang et al. - 2020 - Random Forest Prediction Intervals.pdf:application/pdf},
}

@inproceedings{zaffran_conformal_2023,
	title = {Conformal {Prediction} with {Missing} {Values}},
	url = {https://proceedings.mlr.press/v202/zaffran23a.html},
	abstract = {Conformal prediction is a theoretically grounded framework for constructing predictive intervals. We study conformal prediction with missing values in the covariates – a setting that brings new challenges to uncertainty quantification. We first show that the marginal coverage guarantee of conformal prediction holds on imputed data for any missingness distribution and almost all imputation functions. However, we emphasize that the average coverage varies depending on the pattern of missing values: conformal methods tend to construct prediction intervals that under-cover the response conditionally to some missing patterns. This motivates our novel generalized conformalized quantile regression framework, missing data augmentation, which yields prediction intervals that are valid conditionally to the patterns of missing values, despite their exponential number. We then show that a universally consistent quantile regression algorithm trained on the imputed data is Bayes optimal for the pinball risk, thus achieving valid coverage conditionally to any given data point. Moreover, we examine the case of a linear model, which demonstrates the importance of our proposal in overcoming the heteroskedasticity induced by missing values. Using synthetic and data from critical care, we corroborate our theory and report improved performance of our methods.},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zaffran, Margaux and Dieuleveut, Aymeric and Josse, Julie and Romano, Yaniv},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {40578--40604},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/2RPWZUVJ/Zaffran et al. - 2023 - Conformal Prediction with Missing Values.pdf:application/pdf},
}

@article{khosravi_comprehensive_2011,
	title = {Comprehensive review of neural network-based prediction intervals and new advances},
	volume = {22},
	url = {https://ieeexplore.ieee.org/abstract/document/5966350/?casa_token=yWN0CWlNfBAAAAAA:C01j5O_oNXvvflW6tzky4ducBR_UR4Ui2fX2Z_pno7Lz6KPYLNdTcNtfjqLq8mMtxJhEHvOu_7c},
	number = {9},
	urldate = {2024-12-06},
	journal = {IEEE Transactions on neural networks},
	author = {Khosravi, Abbas and Nahavandi, Saeid and Creighton, Doug and Atiya, Amir F.},
	year = {2011},
	note = {Publisher: IEEE},
	pages = {1341--1356},
}

@article{sun_deep_2023,
	title = {Deep learning versus conventional methods for missing data imputation: {A} review and comparative study},
	volume = {227},
	shorttitle = {Deep learning versus conventional methods for missing data imputation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423007030?casa_token=an7D5ODvuDUAAAAA:wB8IDKF_HcWFHx2LCH3groKT6UIFUabiudogFtUbwzKkgMAnM5fHgcvC8GpOaCYCwdp01VVXFTwv},
	urldate = {2025-02-17},
	journal = {Expert Systems with Applications},
	author = {Sun, Yige and Li, Jing and Xu, Yifan and Zhang, Tingting and Wang, Xiaofeng},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {120201},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/W6FVVNIX/S0957417423007030.html:text/html},
}

@inproceedings{yoon_gain_2018,
	title = {{GAIN}: {Missing} {Data} {Imputation} using {Generative} {Adversarial} {Nets}},
	shorttitle = {{GAIN}},
	url = {https://proceedings.mlr.press/v80/yoon18a.html},
	abstract = {We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.},
	language = {en},
	urldate = {2025-02-17},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yoon, Jinsung and Jordon, James and Schaar, Mihaela},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5689--5698},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/XXCYPJQY/Yoon et al. - 2018 - GAIN Missing Data Imputation using Generative Adv.pdf:application/pdf},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {1566-2535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
	doi = {10.1016/j.inffus.2021.05.008},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
	urldate = {2025-02-17},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	keywords = {Machine learning, Artificial intelligence, Deep learning, Bayesian statistics, Ensemble learning, Uncertainty quantification},
	pages = {243--297},
	file = {Full Text:/Users/4243579/Zotero/storage/UPSSI4S8/Abdar et al. - 2021 - A review of uncertainty quantification in deep lea.pdf:application/pdf;ScienceDirect Snapshot:/Users/4243579/Zotero/storage/KGLJDX57/S1566253521001081.html:text/html},
}

@article{shafer_tutorial_2008,
	title = {A tutorial on conformal prediction.},
	volume = {9},
	url = {https://www.jmlr.org/papers/volume9/shafer08a/shafer08a.pdf},
	number = {3},
	urldate = {2025-02-17},
	journal = {Journal of Machine Learning Research},
	author = {Shafer, Glenn and Vovk, Vladimir},
	year = {2008},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/AKPH6DLV/Shafer and Vovk - 2008 - A tutorial on conformal prediction..pdf:application/pdf},
}

@article{rubin_multiple_1996,
	title = {Multiple {Imputation} after 18+ {Years}},
	volume = {91},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476908},
	doi = {10.1080/01621459.1996.10476908},
	language = {en},
	number = {434},
	urldate = {2025-02-17},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B.},
	month = jun,
	year = {1996},
	pages = {473--489},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/BHTX8S3D/Rubin - 1996 - Multiple Imputation after 18+ Years.pdf:application/pdf},
}

@misc{morvan_imputation_2025,
	title = {Imputation for prediction: beware of diminishing returns},
	shorttitle = {Imputation for prediction},
	url = {http://arxiv.org/abs/2407.19804},
	doi = {10.48550/arXiv.2407.19804},
	abstract = {Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying if and when investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, improving imputation only has a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Morvan, Marine Le and Varoquaux, Gaël},
	month = feb,
	year = {2025},
	note = {arXiv:2407.19804 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/4243579/Zotero/storage/HVZKFHA8/Morvan and Varoquaux - 2025 - Imputation for prediction beware of diminishing r.pdf:application/pdf;Snapshot:/Users/4243579/Zotero/storage/INLL65VZ/2407.html:text/html},
}

@inproceedings{morvan_linear_2020,
	title = {Linear predictor on linearly-generated data with missing values: non consistency and solutions},
	shorttitle = {Linear predictor on linearly-generated data with missing values},
	url = {https://proceedings.mlr.press/v108/morvan20a.html},
	abstract = {We consider building predictors when the data have missing values. We study the seemingly-simple case where the target to predict is a linear function of the fully observed data and we show that, in the presence of missing values, the optimal predictor is not linear in general. In the particular Gaussian case, it can be written as a linear function of multiway interactions between the observed data and the various missing value indicators. Due to its intrinsic complexity, we study a simple approximation and prove generalization bounds with finite samples, highlighting regimes for which each method performs best. We then show that multilayer perceptrons with ReLU activation functions can be consistent, and can explore good trade-offs between the true model and approximations. Our study highlights the interesting family of models that are beneficial to fit with missing values depending on the amount of data available.},
	language = {en},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Morvan, Marine Le and Prost, Nicolas and Josse, Julie and Scornet, Erwan and Varoquaux, Gael},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3165--3174},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/LGQ8HRT6/Morvan et al. - 2020 - Linear predictor on linearly-generated data with m.pdf:application/pdf},
}

@article{le_morvan_whatsa_2021,
	title = {What’sa good imputation to predict with missing values?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/5fe8fdc79ce292c39c5f209d734b7206-Abstract.html},
	urldate = {2025-05-08},
	journal = {Advances in Neural Information Processing Systems},
	author = {Le Morvan, Marine and Josse, Julie and Scornet, Erwan and Varoquaux, Gaël},
	year = {2021},
	pages = {11530--11540},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/MSZYNHXW/Le Morvan et al. - 2021 - What’sa good imputation to predict with missing va.pdf:application/pdf},
}

@article{dewolf_valid_2023,
	title = {Valid prediction intervals for regression problems},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10178-5},
	doi = {10.1007/s10462-022-10178-5},
	abstract = {Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the validity and calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. So far, no study has analysed this issue whilst simultaneously considering these four classes of methods. In this independent comparative study, we review the above four classes of methods from a conceptual and experimental point of view in the i.i.d. setting. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.},
	language = {en},
	number = {1},
	urldate = {2025-05-08},
	journal = {Artificial Intelligence Review},
	author = {Dewolf, Nicolas and Baets, Bernard De and Waegeman, Willem},
	month = jan,
	year = {2023},
	keywords = {Calibration, Prediction interval, Regression, Artificial Intelligence, Bayesian network, Conformal prediction, Ensemble theory},
	pages = {577--613},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/XE4I26NR/Dewolf et al. - 2023 - Valid prediction intervals for regression problems.pdf:application/pdf},
}

@article{miles_obtaining_2016,
	title = {Obtaining {Predictions} from {Models} {Fit} to {Multiply} {Imputed} {Data}},
	volume = {45},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124115610345},
	doi = {10.1177/0049124115610345},
	abstract = {Obtaining predictions from regression models fit to multiply imputed data can be challenging because treatments of multiple imputation seldom give clear guidance on how predictions can be calculated, and because available software often does not have built-in routines for performing the necessary calculations. This research note reviews how predictions can be obtained using Rubin?s rules, that is, by being estimated separately in each imputed data set and then combined. It then demonstrates that predictions can also be calculated directly from the final analysis model. Both approaches yield identical results when predictions rely solely on linear transformations of the coefficients and calculate standard errors using the delta method and diverge only slightly when using nonlinear transformations. However, calculation from the final model is faster, easier to implement, and generates predictions with a clearer relationship to model coefficients. These principles are illustrated using data from the General Social Survey and with a simulation.},
	number = {1},
	urldate = {2025-05-20},
	journal = {Sociological Methods \& Research},
	author = {Miles, Andrew},
	month = feb,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {175--185},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/YRTTKV29/Miles - 2016 - Obtaining Predictions from Models Fit to Multiply .pdf:application/pdf},
}

@misc{noauthor_relation_nodate,
	title = {On the {Relation} between {Prediction} and {Imputation} {Accuracy} under {Missing} {Covariates}},
	url = {https://www.mdpi.com/1099-4300/24/3/386},
	urldate = {2025-05-20},
	file = {On the Relation between Prediction and Imputation Accuracy under Missing Covariates:/Users/4243579/Zotero/storage/S6ALN2GH/386.html:text/html},
}

@misc{zaffran_predictive_2024,
	title = {Predictive {Uncertainty} {Quantification} with {Missing} {Covariates}},
	url = {http://arxiv.org/abs/2405.15641},
	doi = {10.48550/arXiv.2405.15641},
	abstract = {Predictive uncertainty quantification is crucial in decision-making problems. We investigate how to adequately quantify predictive uncertainty with missing covariates. A bottleneck is that missing values induce heteroskedasticity on the response's predictive distribution given the observed covariates. Thus, we focus on building predictive sets for the response that are valid conditionally to the missing values pattern. We show that this goal is impossible to achieve informatively in a distribution-free fashion, and we propose useful restrictions on the distribution class. Motivated by these hardness results, we characterize how missing values and predictive uncertainty intertwine. Particularly, we rigorously formalize the idea that the more missing values, the higher the predictive uncertainty. Then, we introduce a generalized framework, coined CP-MDA-Nested*, outputting predictive sets in both regression and classification. Under independence between the missing value pattern and both the features and the response (an assumption justified by our hardness results), these predictive sets are valid conditionally to any pattern of missing values. Moreover, it provides great flexibility in the trade-off between statistical variability and efficiency. Finally, we experimentally assess the performances of CP-MDA-Nested* beyond its scope of theoretical validity, demonstrating promising outcomes in more challenging configurations than independence.},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Zaffran, Margaux and Josse, Julie and Romano, Yaniv and Dieuleveut, Aymeric},
	month = may,
	year = {2024},
	note = {arXiv:2405.15641 [stat]},
	keywords = {Statistics - Methodology},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/X6Q5WYSY/Zaffran et al. - 2024 - Predictive Uncertainty Quantification with Missing.pdf:application/pdf;Snapshot:/Users/4243579/Zotero/storage/PA4VKHCQ/2405.html:text/html},
}

@article{bartlett_multiple_2015,
	title = {Multiple imputation of covariates by fully conditional specification: {Accommodating} the substantive model},
	volume = {24},
	issn = {0962-2802},
	shorttitle = {Multiple imputation of covariates by fully conditional specification},
	url = {https://doi.org/10.1177/0962280214521348},
	doi = {10.1177/0962280214521348},
	abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
	number = {4},
	urldate = {2025-05-21},
	journal = {Statistical Methods in Medical Research},
	author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
	month = aug,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {462--487},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/DZ8LGBCS/Bartlett et al. - 2015 - Multiple imputation of covariates by fully conditi.pdf:application/pdf},
}

@article{meng_multiple-imputation_1994,
	title = {Multiple-{Imputation} {Inferences} with {Uncongenial} {Sources} of {Input}},
	volume = {9},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2246252},
	abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
	number = {4},
	urldate = {2025-05-21},
	journal = {Statistical Science},
	author = {Meng, Xiao-Li},
	year = {1994},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {538--558},
}

@article{dagostino_mcgowan_why_2024,
	title = {The “{Why}” behind including “{Y}” in your imputation model},
	volume = {33},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/09622802241244608},
	doi = {10.1177/09622802241244608},
	abstract = {Missing data is a common challenge when analyzing epidemiological data, and imputation is often used to address this issue. Here, we investigate the scenario where a covariate used in an analysis has missingness and will be imputed. There are recommendations to include the outcome from the analysis model in the imputation model for missing covariates, but it is not necessarily clear if this recommendation always holds and why this is sometimes true. We examine deterministic imputation (i.e. single imputation with fixed values) and stochastic imputation (i.e. single or multiple imputation with random values) methods and their implications for estimating the relationship between the imputed covariate and the outcome. We mathematically demonstrate that including the outcome variable in imputation models is not just a recommendation but a requirement to achieve unbiased results when using stochastic imputation methods. Moreover, we dispel common misconceptions about deterministic imputation models and demonstrate why the outcome should not be included in these models. This article aims to bridge the gap between imputation in theory and in practice, providing mathematical derivations to explain common statistical recommendations. We offer a better understanding of the considerations involved in imputing missing covariates and emphasize when it is necessary to include the outcome variable in the imputation model.},
	number = {6},
	urldate = {2025-06-11},
	journal = {Statistical Methods in Medical Research},
	author = {D’Agostino McGowan, Lucy and Lotspeich, Sarah C and Hepler, Staci A},
	month = jun,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {996--1020},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/ASL4EY2X/D’Agostino McGowan et al. - 2024 - The “Why” behind including “Y” in your imputation .pdf:application/pdf},
}

@article{barber_predictive_2021,
	title = {Predictive inference with the jackknife+},
	volume = {49},
	url = {https://www.jstor.org/stable/27028783},
	number = {1},
	urldate = {2025-06-18},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	year = {2021},
	note = {Publisher: JSTOR},
	pages = {486--507},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/YK9TTSC5/Barber et al. - 2021 - Predictive inference with the jackknife+.pdf:application/pdf},
}

@article{miles_obtaining_2016-1,
	title = {Obtaining {Predictions} from {Models} {Fit} to {Multiply} {Imputed} {Data}},
	volume = {45},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124115610345},
	doi = {10.1177/0049124115610345},
	abstract = {Obtaining predictions from regression models fit to multiply imputed data can be challenging because treatments of multiple imputation seldom give clear guidance on how predictions can be calculated, and because available software often does not have built-in routines for performing the necessary calculations. This research note reviews how predictions can be obtained using Rubin’s rules, that is, by being estimated separately in each imputed data set and then combined. It then demonstrates that predictions can also be calculated directly from the final analysis model. Both approaches yield identical results when predictions rely solely on linear transformations of the coefficients and calculate standard errors using the delta method and diverge only slightly when using nonlinear transformations. However, calculation from the final model is faster, easier to implement, and generates predictions with a clearer relationship to model coefficients. These principles are illustrated using data from the General Social Survey and with a simulation.},
	language = {EN},
	number = {1},
	urldate = {2025-08-13},
	journal = {Sociological Methods \& Research},
	author = {Miles, Andrew},
	month = feb,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {175--185},
}

@article{barber_predictive_2021-1,
	title = {Predictive {Inference} with the {Jackknife}+},
	volume = {49},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/27028783},
	abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9?28) and we discuss connections.},
	number = {1},
	urldate = {2025-08-13},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Cand�s, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {486--507},
	file = {JSTOR Full Text PDF:/Users/4243579/Zotero/storage/R6EIXWCT/Barber et al. - 2021 - Predictive Inference with the Jackknife+.pdf:application/pdf},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2025-09-10},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {machine learning, data mining, supervised learning, unsupervised learning, Averaging, Boosting, classification, clustering, Projection pursuit, Random Forest, Support Vector Machine},
	file = {Full Text PDF:/Users/4243579/Zotero/storage/DGJCZ88I/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf:application/pdf},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	url = {https://academic.oup.com/biomet/article-abstract/63/3/581/270932},
	number = {3},
	urldate = {2025-09-24},
	journal = {Biometrika},
	author = {Rubin, Donald B.},
	year = {1976},
	note = {Publisher: Oxford University Press},
	pages = {581--592},
	file = {Available Version (via Google Scholar):/Users/4243579/Zotero/storage/U7DJMWND/Rubin - 1976 - Inference and missing data.pdf:application/pdf},
}
